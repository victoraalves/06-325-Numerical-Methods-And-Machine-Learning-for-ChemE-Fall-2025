{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd57328a",
   "metadata": {},
   "source": [
    "# Lecture 2 — Nonlinear systems - Root finding\n",
    "\n",
    "\n",
    "\n",
    "Today we will cover nonlinear equations and systems of nonlinear equations (single and multivariable)\n",
    "\n",
    "## Learning Outcomes\n",
    "By the end of this lecture you will be able to:\n",
    "- Explain why many engineering problems described by a nonlinear equation (or a system) can be seeing to solving **f(x)=0** or **F(x)=0**.\n",
    "\n",
    "- Understand **convergence/stop** criteria (function tolerance, step tolerance, iteration cap).\n",
    "\n",
    "- Distinguish **bracketed** (bisection) vs **open** (Newton) root-finding numerical methods.\n",
    "\n",
    "- Implement bisection and Newton methods from scratch, as well as use `scipy.optimize` equivalents.\n",
    "\n",
    "- Understand what can go wrong when employing these methods and how to circumvent such challenges.\n",
    "\n",
    "- Transition from single to multivariable, system of equations root-finding.\n",
    "\n",
    "- Be able to use `scipy.optimize.root` to solve systems of nonlinear equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7514167",
   "metadata": {},
   "source": [
    "## 1) Root finding: exact vs numerical; errors & tolerance\n",
    "- **Exact** vs **numerical**: most ChemE models need numerical roots. There is no analytical (closed-form) solution to the expression (or set of expressions) that arise. We saw some examples last lecture. A simple example (not ChemE) is:\n",
    "\n",
    "    $f(x) = ℯ^{-x}+ 0.1 \\, \\,x \\,\\, sin(x) = 0$\n",
    "\n",
    "\n",
    "\n",
    "This nonlinear equation has no simple algebraic solution for $x$. Instead, numerical root-finding methods are used to determine the *approximate solution*. That is, within a tolerance margin.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- We should reduce/simplify the system at hand to **f(x)=0** (or **F(x)=0** in multivariable case, uppercase here means a vector function).\n",
    "- A numerical method needs tolerances to iterate towards a solution. For example: \n",
    "\n",
    "    Function ($|f(x_k)|<\\varepsilon_f$), step ($|x_{k+1}-x_k|<\\varepsilon_x$), and maximum number of iterations are alternatives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74639c29",
   "metadata": {},
   "source": [
    "We stop when one (or more) conditions are met:\n",
    "- **Function tolerance**: $|f(x_k)| < \\varepsilon_f$ — we’re close to a zero/root.\n",
    "- **Step/interval tolerance**: $|x_{k+1}-x_k| < \\varepsilon_x$ or **bisection** width $(b-a) < \\varepsilon_x$.\n",
    "- **Iteration cap**: `max_iter` — prevents infinite loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4960033",
   "metadata": {},
   "source": [
    "## 2) Interval Halving Methods: Bisection\n",
    "**Idea.** With a sign-change bracket $[a,b]$ for a continuous $f$, a root exists inside this bracket. This is called the **intermediate value theorem**.\n",
    "\n",
    "\n",
    "If we halve repeatedly and keep the sign-change subinterval, we will find a solution eventually!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad4c33",
   "metadata": {},
   "source": [
    "### Bisection Visual Exploration\n",
    "\n",
    "Let's use an interactive animation to better understand how interval halving methods, such as Bisection, work. \n",
    "\n",
    "If you are seeing this in the website version of the lecture, just open this notebook in your google colab session or locally in your machine, by downloading the *.ipynb file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b28f0",
   "metadata": {},
   "source": [
    "The example is a nonlinear function defined as $y(x) = e^{-x} - 0.1sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce139b3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bff0130b33474da0a9d95e2fd026c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=5.0, description='a', layout=Layout(width='360px'), max=10.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Button, IntText, FloatSlider, Checkbox, Output\n",
    "from IPython.display import display, clear_output\n",
    "plt.rcParams.update({'figure.figsize': (7.6,5.0), 'axes.labelsize':13, 'axes.titlesize':14})\n",
    "\n",
    "def f_bisect(x): \n",
    "    return np.exp(-x) - 0.1*x*np.sin(x)\n",
    "\n",
    "def compute_bisection_brackets(f, a, b, nsteps=25, stop_on_exact=True):\n",
    "    # ensure a <= b\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "\n",
    "    aa, bb = float(a), float(b)\n",
    "    fa, fb = f(aa), f(bb)\n",
    "    out = []\n",
    "\n",
    "    # endpoint root?\n",
    "    if fa == 0 or fb == 0:\n",
    "        mid = aa if fa == 0 else bb\n",
    "        out.append((aa, bb, mid))\n",
    "        return out\n",
    "\n",
    "    # need a strict sign change to start\n",
    "    if fa * fb > 0:\n",
    "        return out  # invalid bracket\n",
    "\n",
    "    for _ in range(int(nsteps)):\n",
    "        mid = 0.5 * (aa + bb)\n",
    "        out.append((aa, bb, mid))\n",
    "        fm = f(mid)\n",
    "\n",
    "        if stop_on_exact and fm == 0:\n",
    "            break\n",
    "\n",
    "        # keep the subinterval that preserves the sign change\n",
    "        if fa * fm < 0:\n",
    "            bb, fb = mid, fm\n",
    "        else:\n",
    "            aa, fa = mid, fm\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def bisection_buttons_widget():\n",
    "    a_slider = FloatSlider(description='a', value=5.0, min=0.5, max=10.0, step=0.01, layout=widgets.Layout(width='360px'), style={'description_width':'15px'})\n",
    "    b_slider = FloatSlider(description='b', value=8.0, min=0.5, max=10.0, step=0.01, layout=widgets.Layout(width='360px'), style={'description_width':'15px'})\n",
    "    steps_text = IntText(description='max steps', value=20, layout=widgets.Layout(width='180px'), style={'description_width':'90px'})\n",
    "    show_hist = Checkbox(value=True, description='Show history', indent=False)\n",
    "\n",
    "    btn_prev = Button(description='Prev', layout=widgets.Layout(width='120px', height='36px'), disabled=True)\n",
    "    btn_next = Button(description='Next', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    btn_reset = Button(description='Reset', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    iter_text = IntText(description='iter', value=0, layout=widgets.Layout(width='140px'), style={'description_width':'38px'})\n",
    "\n",
    "    out = Output(layout=widgets.Layout(width='880px', height='520px', border='1px solid #ddd'))\n",
    "    state={'br':[], 'i':0}\n",
    "\n",
    "    def recompute():\n",
    "        state['br']=compute_bisection_brackets(f_bisect, a_slider.value, b_slider.value, int(steps_text.value))\n",
    "        state['i']=0; iter_text.value=0; btn_prev.disabled=True; btn_next.disabled=(len(state['br'])<=1)\n",
    "\n",
    "    def render():\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            x=np.linspace(0.5,10,600); y=f_bisect(x)\n",
    "            fig,ax=plt.subplots(); ax.axhline(0,color='k',lw=1); ax.plot(x,y,'b-',label='f(x)')\n",
    "            ax.set_xlim(0.5,11); ax.set_ylim(-2, 2); ax.grid(True)\n",
    "            br=state['br']\n",
    "            if not br:\n",
    "                ax.text(0.5,0.9,'Invalid bracket: f(a)*f(b) ≥ 0', transform=ax.transAxes, color='crimson',\n",
    "                        bbox=dict(facecolor='mistyrose', edgecolor='crimson')); ax.set_title('Bisection (invalid bracket)'); plt.show(); return\n",
    "            i=state['i']; ai,bi,mi=br[i]\n",
    "            if show_hist.value and i>0:\n",
    "                for (ap,bp,_) in br[:i]:\n",
    "                    ax.axvspan(ap,bp,color='grey',alpha=0.08); ax.axvline(ap,color='grey',ls='--',lw=1); ax.axvline(bp,color='grey',ls='--',lw=1)\n",
    "            ax.axvspan(ai,bi,color='gold',alpha=0.25,label='current bracket')\n",
    "            ax.axvline(ai,color='orange',ls='--',lw=2); ax.axvline(bi,color='orange',ls='--',lw=2)\n",
    "            ax.plot(mi,f_bisect(mi),'ro',label='midpoint')\n",
    "            ax.set_title('Bisection: interval halving')\n",
    "            ax.set_xlabel(f'iter={i}, a={ai:.4f}, b={bi:.4f}, mid={mi:.4f}, |f(mid)|={abs(f_bisect(mi)):.2e}')\n",
    "            ax.legend(loc='upper left'); plt.show()\n",
    "\n",
    "    def on_next(_):\n",
    "        if state['i'] < max(0, len(state['br'])-1):\n",
    "            state['i']+=1; iter_text.value=state['i']; btn_prev.disabled=False\n",
    "            if state['i']>=len(state['br'])-1: btn_next.disabled=True\n",
    "            render()\n",
    "\n",
    "    def on_prev(_):\n",
    "        if state['i']>0:\n",
    "            state['i']-=1; iter_text.value=state['i']; btn_next.disabled=False\n",
    "            if state['i']<=0: btn_prev.disabled=True\n",
    "            render()\n",
    "\n",
    "    def on_reset(_):\n",
    "        state['i']=0; iter_text.value=0; btn_prev.disabled=True; btn_next.disabled=(len(state['br'])<=1); render()\n",
    "\n",
    "    def on_param_change(change):\n",
    "        recompute(); render()\n",
    "\n",
    "    btn_next.on_click(on_next); btn_prev.on_click(on_prev); btn_reset.on_click(on_reset)\n",
    "    a_slider.observe(on_param_change, names='value'); b_slider.observe(on_param_change, names='value')\n",
    "    steps_text.observe(on_param_change, names='value'); show_hist.observe(on_param_change, names='value')\n",
    "    recompute(); render()\n",
    "    display(VBox([HBox([a_slider,b_slider,steps_text,show_hist]), HBox([btn_prev,btn_next,btn_reset,iter_text]), out]))\n",
    "\n",
    "bisection_buttons_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f3106",
   "metadata": {},
   "source": [
    "How does this in code looks like? Let's see one possible implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6ecc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def bisection(f, a, b, ftol=1e-10, xtol=1e-12, max_iter=200):\n",
    "\n",
    "    \"\"\"\n",
    "    Bisection's Method.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    f: function\n",
    "        The function we want to find the root.\n",
    "    \n",
    "    a: float\n",
    "        Lower bound of the initial interval.\n",
    "    b: float\n",
    "        Upper bound of the initial interval.\n",
    "    ftol: float\n",
    "        Tolerance for absolute function value (default: 1e-10)\n",
    "    xtol: float\n",
    "        Tolerance for the step length (default: 1e-12)\n",
    "    max_iter: int\n",
    "        Max number of iterations (default: 200)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m: float\n",
    "        Approximate root of f(x)\n",
    "    iter: int\n",
    "        Number of iterations.\n",
    "    \n",
    "    fm: float\n",
    "        Value of f(x) at the solution.    \n",
    "    \"\"\"\n",
    "    fa = f(a)\n",
    "    fb = f(b)\n",
    "\n",
    "    if fa * fb > 0:\n",
    "        raise ValueError(\"f(a)*f(b)>0, hence we don't have a sign change. Exit.\")\n",
    "\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        m = 0.5 * (a + b)\n",
    "\n",
    "        fm = f(m)\n",
    "\n",
    "        if np.abs(fm) < ftol or np.abs((b-a)) < xtol:\n",
    "            return m, k+1, fm\n",
    "\n",
    "        if fa * fm < 0:\n",
    "            b, fb = m, fm\n",
    "        else:\n",
    "            a, fa = m, fm\n",
    "    \n",
    "    m =  0.5 * (a + b)\n",
    "\n",
    "    fm = f(m)\n",
    "\n",
    "\n",
    "    return m, max_iter, fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d70f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.286147252423689, 32, np.float64(-2.1169849293706244e-11))\n"
     ]
    }
   ],
   "source": [
    "def f_bisect(x): \n",
    "    return np.exp(-x) - 0.1*x*np.sin(x)\n",
    "\n",
    "a = 5.00\n",
    "b = 8.00\n",
    "solution = bisection(f_bisect, a, b)\n",
    "\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cd391",
   "metadata": {},
   "source": [
    "A few notes regarding the bisection method:\n",
    "\n",
    "\n",
    "\n",
    "- We will always find a solution, given that the intermediate value theorem is true when choosing the interval $[a,b]$\n",
    "- Bisection can be a bit slow to converge. It has linear convergence.\n",
    "\n",
    "This leaves an open question of \"can we have faster methods?\" Let's see it below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94283e33",
   "metadata": {},
   "source": [
    "## 3) Open Methods: Newton's Method\n",
    "\n",
    "In the open methods, we don't need an interval $[a,b]$ to try to find a solution. Instead, we need only an initial estimate $x_0$. This might look an absolute improvement at a first glance, since these methods tend to converge faster in general, but there are a few caveats. First, let's take a look at the iterative equation for Newton's method:\n",
    "\n",
    "**Newton's method formula:** $x_{k+1}=x_k-\\dfrac{f(x_k)}{f'(x_k)}$ \n",
    "\n",
    "How do we even got this formula in the first place? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fceb4fa",
   "metadata": {},
   "source": [
    "### Newton’s Method derivation via Taylor Series Expansion (TSE)\n",
    "\n",
    "#### Reminder: What is a Taylor Series?\n",
    "If a function $f(x)$ is sufficiently smooth (that is, we can take derivatives), we are able to approximate it near a point, say, $x=a$ by its derivatives at $a$:\n",
    "\n",
    "$f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\frac{f^{(3)}(a)}{3!}(x-a)^3 + \\cdots$\n",
    "- The first term is the value of $f$ at $a$.\n",
    "\n",
    "- The second term adds the local linear (slope) correction.\n",
    "\n",
    "- Higher-order terms capture curvature and what we call \"high-order\" effects.\n",
    "\n",
    "- When $x$ is close to $a$, the higher-order terms are small.\n",
    "\n",
    "\n",
    "\n",
    "#### Apply Taylor Expansion to Root Finding\n",
    "Remember our goal: solve $f(x)=0$ ! (exclamation mark, not factorial :) ) \n",
    "\n",
    "Let $x_k$ be the current guess. Expand $f$ with a $1^{st}$ order TSE expansion around $x_k$:\n",
    "$f(x) \\approx f(x_k) + f'(x_k)(x - x_k)$\n",
    "\n",
    "We know $x_k$ and hence, can calculate $f(x_k)$ and $f'(x_k)$\n",
    "\n",
    "#### Impose the Root Condition\n",
    "At the next iterate $x_{k+1}$ we want $f(x_{k+1})=0$. After all, we want it to converge, correct? \n",
    "\n",
    "Let's impose this condition then. $f(x_{k+1})=0$:\n",
    "\n",
    "$(x_{k+1})=0 \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$\n",
    "\n",
    "\n",
    "Solve for $x_{k+1}$ to get the Newton update (also called newton step):\n",
    "\n",
    "\n",
    "$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$\n",
    "\n",
    "This is also written sometimes as \n",
    "\n",
    "$x_{k+1} = x_k + d_k$ \n",
    "\n",
    "$d_k = - \\frac{f(x_k)}{f'(x_k)}$\n",
    "\n",
    "\n",
    "#### Why is Newton’s method fast?\n",
    "\n",
    "Let the root be $r$ and define the error $e_k = x_k - r$\n",
    "\n",
    "Start from the **full Taylor series of $f$ about $r$** (with $x_k = r + e_k$):\n",
    "\n",
    "\n",
    "$$\n",
    "f(x_k) = f(r+e_k)\n",
    "      = f(r) + f'(r)e_k + \\frac{f''(r)}{2!}e_k^2 + \\frac{f^{(3)}(r)}{3!}e_k^3 + \\cdots\n",
    "$$\n",
    "\n",
    "Since $f(r)=0$, this simplifies to\n",
    "\n",
    "\n",
    "$$\n",
    "f(x_k) = f'(r)e_k + \\frac{f''(r)}{2}e_k^2 + \\frac{f^{(3)}(r)}{6}e_k^3 + \\cdots\n",
    "$$\n",
    "\n",
    "\n",
    "If $x_k$ is close to $r$ (so $|e_k|$ is small), higher-order terms are negligible, giving\n",
    "\n",
    "\n",
    "$$\n",
    "f(x_k) \\approx f'(r)e_k + \\tfrac{1}{2} f''(r)e_k^2\n",
    "$$\n",
    "\n",
    "Newton’s update is\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n",
    "$$\n",
    "\n",
    "Hence the next error is\n",
    "\n",
    "$$\n",
    "e_{k+1} = x_{k+1} - r = e_k - \\frac{f(x_k)}{f'(x_k)}\n",
    "$$\n",
    "\n",
    "Substitute the Taylor approximation for $f(x_k)$ and (near $r$) replace $f'(x_k)$ by $f'(r)$:\n",
    "\n",
    "$$f'(x_k) = f'(r + e_k)\n",
    "= f'(r) + f''(r)e_k + \\tfrac12 f^{(3)}(r)e_k^2 + \\cdots \\approx f'(r)\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_{k+1} \\approx e_k - \\frac{f'(r)e_k + \\tfrac{1}{2}f''(r)e_k^2}{f'(r)}\n",
    "          = e_k - (e_k - \\frac{f''(r)}{2f'(r)} e_k^2)\n",
    "          = \\frac{f''(r)}{2f'(r)} e_k^2\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "e_{k+1} \\propto e_k^2\n",
    "$$\n",
    "\n",
    "The new error is proportional to the **square** of the old error (quadratic convergence). \n",
    " \n",
    "If $e_k \\sim 10^{-2}$, then $e_{k+1} \\sim 10^{-4}$.  \n",
    "\n",
    "By contrast, bisection has only linear convergence: $e_{k+1} \\approx \\tfrac{1}{2}e_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dabd56",
   "metadata": {},
   "source": [
    "### Newton's Method Visual Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4523292",
   "metadata": {},
   "source": [
    "This works in your jupyter notebook sesssion. Not in the website. Let's explore Newton's method! \n",
    "\n",
    "Again, you should not worry about the code that generated the anmiations below. It's just the use of [jupyter widgets](https://ipywidgets.readthedocs.io/en/latest/) (and a bit of prompting using ChatGPT to get the widget working well) so we could generate an interactive visualization of how Newton's method works :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba18d822",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b000c134fc452c85675d791f65e986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=1.5, description='x0', layout=Layout(width='360px'), max=5.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Button, IntText, FloatSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Clean, well-behaved cubic: roots at 1, 2, 4\n",
    "def f(x):\n",
    "    return x**3 - 7*x**2 + 14*x - 8\n",
    "\n",
    "def fprime(x):\n",
    "    return 3*x**2 - 14*x + 14\n",
    "\n",
    "def newton_sequence(f, fprime, x0, max_steps=25, ftol=1e-12, xtol=1e-12):\n",
    "    xs = [float(x0)]\n",
    "    for _ in range(int(max_steps)):\n",
    "        x = xs[-1]\n",
    "        fx = f(x)\n",
    "        dfx = fprime(x)\n",
    "        if abs(dfx) < 1e-14:   # safeguard against tiny slope\n",
    "            break\n",
    "        xnew = x - fx / dfx\n",
    "        xs.append(xnew)\n",
    "        # test convergence at the new iterate\n",
    "        if abs(f(xnew)) < ftol or abs(xnew - x) < xtol:\n",
    "            break\n",
    "    return xs\n",
    "\n",
    "def newton_buttons_widget():\n",
    "    plt.rcParams.update({'figure.figsize': (7.6,5.0), 'axes.labelsize':13, 'axes.titlesize':14})\n",
    "\n",
    "    x0_slider = FloatSlider(description='x0', value=1.5, min=0.0, max=5.0, step=0.01,\n",
    "                            layout=widgets.Layout(width='360px'), style={'description_width':'20px'})\n",
    "    steps_text = IntText(description='max steps', value=10,\n",
    "                         layout=widgets.Layout(width='180px'), style={'description_width':'90px'})\n",
    "    btn_prev = Button(description='Prev', layout=widgets.Layout(width='120px', height='36px'), disabled=True)\n",
    "    btn_next = Button(description='Next', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    btn_reset = Button(description='Reset', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    iter_text = IntText(description='iter', value=0,\n",
    "                        layout=widgets.Layout(width='140px'), style={'description_width':'38px'})\n",
    "    out = Output(layout=widgets.Layout(width='880px', height='520px', border='1px solid #ddd'))\n",
    "\n",
    "    state = {'xs': [], 'i': 0}\n",
    "\n",
    "    def recompute():\n",
    "        state['xs'] = newton_sequence(f, fprime, x0_slider.value, int(steps_text.value))\n",
    "        state['i'] = 0\n",
    "        iter_text.value = 0\n",
    "        btn_prev.disabled = True\n",
    "        btn_next.disabled = (len(state['xs']) <= 1)\n",
    "\n",
    "    def render():\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            X = np.linspace(0, 5, 600); Y = f(X)\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.axhline(0, color='k', lw=1)\n",
    "            ax.plot(X, Y, 'b-', label='f(x)')\n",
    "            ax.set_xlim(0, 5)\n",
    "\n",
    "            # dynamic y-limits with margin\n",
    "            yspan = Y.max() - Y.min()\n",
    "            pad = 0.08*yspan if yspan > 0 else 1.0\n",
    "            ax.set_ylim(Y.min()-pad, Y.max()+pad)\n",
    "            ax.grid(True)\n",
    "\n",
    "            # mark true roots for reference\n",
    "            for r in (1.0, 2.0, 4.0):\n",
    "                ax.axvline(r, color='orange', ls='--', lw=1.2, alpha=0.85)\n",
    "                ax.text(r, 0.02*(Y.max()-Y.min())+0, f\"x={r:g}\", color='orange', ha='center')\n",
    "\n",
    "            xs = state['xs']\n",
    "\n",
    "            i = state['i']; xn = xs[i]; yn = f(xn)\n",
    "            ax.plot(xn, yn, 'ro', label='current iterate')\n",
    "\n",
    "            if i < len(xs) - 1:\n",
    "                slope = fprime(xn)\n",
    "                tangent = yn + slope*(X - xn)\n",
    "                ax.plot(X, tangent, 'g--', lw=1.4, label='tangent at $x_n$')\n",
    "                ax.axvline(xs[i+1], color='purple', ls='--', lw=1.5)\n",
    "                ax.plot(xs[i+1], 0, 'mo', label='$x_{n+1}$')\n",
    "\n",
    "            ax.set_title(\"Newton's Method on $x^3 - 7x^2 + 14x - 8$\")\n",
    "            yn_abs = np.abs(yn)\n",
    "            ax.set_xlabel(f'iter={i}, x={xn:.6f}, |f(x)|={yn_abs:.2e}')\n",
    "            ax.legend(loc='upper right')\n",
    "            plt.show()\n",
    "\n",
    "    def on_next(_):\n",
    "        if state['i'] < max(0, len(state['xs']) - 1):\n",
    "            state['i'] += 1\n",
    "            iter_text.value = state['i']\n",
    "            btn_prev.disabled = False\n",
    "            if state['i'] >= len(state['xs']) - 1:\n",
    "                btn_next.disabled = True\n",
    "            render()\n",
    "\n",
    "    def on_prev(_):\n",
    "        if state['i'] > 0:\n",
    "            state['i'] -= 1\n",
    "            iter_text.value = state['i']\n",
    "            btn_next.disabled = False\n",
    "            if state['i'] <= 0:\n",
    "                btn_prev.disabled = True\n",
    "            render()\n",
    "\n",
    "    def on_reset(_):\n",
    "        state['i'] = 0\n",
    "        iter_text.value = 0\n",
    "        btn_prev.disabled = True\n",
    "        btn_next.disabled = (len(state['xs']) <= 1)\n",
    "        render()\n",
    "\n",
    "    def on_param_change(change):\n",
    "        recompute(); render()\n",
    "\n",
    "    btn_next.on_click(on_next)\n",
    "    btn_prev.on_click(on_prev)\n",
    "    btn_reset.on_click(on_reset)\n",
    "    x0_slider.observe(on_param_change, names='value')\n",
    "    steps_text.observe(on_param_change, names='value')\n",
    "\n",
    "    recompute(); render()\n",
    "    display(VBox([HBox([x0_slider, steps_text]), HBox([btn_prev, btn_next, btn_reset, iter_text]), out]))\n",
    "\n",
    "newton_buttons_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc8b77",
   "metadata": {},
   "source": [
    "An example in which things can go wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855661c6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd16e4344f04bb7be60ed9d2a2a693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=1.1, description='x0', layout=Layout(width='360px'), max=4.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Button, IntText, FloatSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# tanh example (can diverge for some x0, e.g., 1.1)\n",
    "def f(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def fprime(x):\n",
    "    t = np.tanh(x)\n",
    "    return 1.0 - t*t  # sech^2(x)\n",
    "\n",
    "def newton_sequence(f, fprime, x0, max_steps=25, ftol=1e-12, xtol=1e-12):\n",
    "    xs = [float(x0)]\n",
    "    for _ in range(int(max_steps)):\n",
    "        x = xs[-1]\n",
    "        fx = f(x)\n",
    "        dfx = fprime(x)\n",
    "        if abs(dfx) < 1e-14:   # safeguard against tiny slope\n",
    "            break\n",
    "        xnew = x - fx / dfx\n",
    "        xs.append(xnew)\n",
    "        # test convergence at the new iterate\n",
    "        if abs(f(xnew)) < ftol or abs(xnew - x) < xtol:\n",
    "            break\n",
    "    return xs\n",
    "\n",
    "def newton_buttons_widget():\n",
    "    plt.rcParams.update({'figure.figsize': (7.6,5.0), 'axes.labelsize':13, 'axes.titlesize':14})\n",
    "\n",
    "    x0_slider = FloatSlider(description='x0', value=1.1, min=-4.0, max=4.0, step=0.01,\n",
    "                            layout=widgets.Layout(width='360px'), style={'description_width':'20px'})\n",
    "    steps_text = IntText(description='max steps', value=10,\n",
    "                         layout=widgets.Layout(width='180px'), style={'description_width':'90px'})\n",
    "    btn_prev = Button(description='Prev', layout=widgets.Layout(width='120px', height='36px'), disabled=True)\n",
    "    btn_next = Button(description='Next', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    btn_reset = Button(description='Reset', layout=widgets.Layout(width='120px', height='36px'))\n",
    "    iter_text = IntText(description='iter', value=0,\n",
    "                        layout=widgets.Layout(width='140px'), style={'description_width':'38px'})\n",
    "    out = Output(layout=widgets.Layout(width='880px', height='520px', border='1px solid #ddd'))\n",
    "\n",
    "    state = {'xs': [], 'i': 0}\n",
    "\n",
    "    def recompute():\n",
    "        state['xs'] = newton_sequence(f, fprime, x0_slider.value, int(steps_text.value))\n",
    "        state['i'] = 0\n",
    "        iter_text.value = 0\n",
    "        btn_prev.disabled = True\n",
    "        btn_next.disabled = (len(state['xs']) <= 1)\n",
    "\n",
    "    def render():\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            X = np.linspace(-4, 4, 600); Y = f(X)\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.axhline(0, color='k', lw=1)\n",
    "            ax.plot(X, Y, 'b-', label='f(x) = tanh(x)')\n",
    "            ax.set_xlim(-4, 4)\n",
    "\n",
    "            # dynamic y-limits with margin (same style as cubic demo)\n",
    "            yspan = Y.max() - Y.min()\n",
    "            pad = 0.08*yspan if yspan > 0 else 1.0\n",
    "            ax.set_ylim(Y.min()-pad, Y.max()+pad)\n",
    "            ax.grid(True)\n",
    "\n",
    "            xs = state['xs']\n",
    "            i = state['i']; xn = xs[i]; yn = f(xn)\n",
    "            ax.plot(xn, yn, 'ro', label='current iterate')\n",
    "\n",
    "            if i < len(xs) - 1:\n",
    "                slope = fprime(xn)\n",
    "                tangent = yn + slope*(X - xn)\n",
    "                ax.plot(X, tangent, 'g--', lw=1.4, label='tangent at $x_n$')\n",
    "                ax.axvline(xs[i+1], color='purple', ls='--', lw=1.5)\n",
    "                ax.plot(xs[i+1], 0, 'mo', label='$x_{n+1}$')\n",
    "\n",
    "            ax.set_title(\"Newton's Method on $\\\\tanh(x)$\")\n",
    "            ax.set_xlabel(f'iter={i}, x={xn:.6f}, |f(x)|={abs(yn):.2e}')\n",
    "            ax.legend(loc='upper right')\n",
    "            plt.show()\n",
    "\n",
    "    def on_next(_):\n",
    "        if state['i'] < max(0, len(state['xs']) - 1):\n",
    "            state['i'] += 1\n",
    "            iter_text.value = state['i']\n",
    "            btn_prev.disabled = False\n",
    "            if state['i'] >= len(state['xs']) - 1:\n",
    "                btn_next.disabled = True\n",
    "            render()\n",
    "\n",
    "    def on_prev(_):\n",
    "        if state['i'] > 0:\n",
    "            state['i'] -= 1\n",
    "            iter_text.value = state['i']\n",
    "            btn_next.disabled = False\n",
    "            if state['i'] <= 0:\n",
    "                btn_prev.disabled = True\n",
    "            render()\n",
    "\n",
    "    def on_reset(_):\n",
    "        state['i'] = 0\n",
    "        iter_text.value = 0\n",
    "        btn_prev.disabled = True\n",
    "        btn_next.disabled = (len(state['xs']) <= 1)\n",
    "        render()\n",
    "\n",
    "    def on_param_change(change):\n",
    "        recompute(); render()\n",
    "\n",
    "    btn_next.on_click(on_next)\n",
    "    btn_prev.on_click(on_prev)\n",
    "    btn_reset.on_click(on_reset)\n",
    "    x0_slider.observe(on_param_change, names='value')\n",
    "    steps_text.observe(on_param_change, names='value')\n",
    "\n",
    "    recompute(); render()\n",
    "    display(VBox([HBox([x0_slider, steps_text]), HBox([btn_prev, btn_next, btn_reset, iter_text]), out]))\n",
    "\n",
    "newton_buttons_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b10f50",
   "metadata": {},
   "source": [
    "It quickly diverges! If we change the initial step slightly, say, $x_0=0.9$, it converges. Try it yourself in the interface above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeab363",
   "metadata": {},
   "source": [
    "###   Implementation of Newton’s Method (scalar case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25200ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def newton(f, fprime, x0, tol=1e-8, max_iter = 50):\n",
    "    \"\"\"\n",
    "    Newton's method.\n",
    "\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "\n",
    "    f: function\n",
    "        The function we want to determine the root.\n",
    "\n",
    "    fprime: function\n",
    "        The derivative of the function f\n",
    "    x0: float\n",
    "        Initial estimate (guess).\n",
    "    tol: float\n",
    "        Tolerance.\n",
    "    max_iter:\n",
    "        Maximum number of iterations...\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    x: float\n",
    "        Approximate root of f(x)\n",
    "    iter: int\n",
    "        Number of iterations\n",
    "    sol: float\n",
    "        Value of f(x) at the solution.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if np.abs(f(x)) < tol:\n",
    "        return x\n",
    "    \n",
    "    # Main for loop:\n",
    "\n",
    "    for k in range(max_iter):\n",
    "\n",
    "        x_new = x - f(x)/fprime(x)  # Newton's update formula\n",
    "\n",
    "        if np.abs(f(x_new)) < tol: # Function value small enough. stop.\n",
    "            return x_new, k+1, np.abs(f(x_new))\n",
    "        if np.abs(x_new - x) < 1e-14: # Step size small enough. Stop.\n",
    "            ## TODO: Maybe we should add 1e-14 as a keyword argument instead of hard coding it.\n",
    "            return x_new, k+1, np.abs(f(x_new))\n",
    "        \n",
    "        x = x_new\n",
    "\n",
    "    \n",
    "    fx = f(x)\n",
    "    x = x_new\n",
    "\n",
    "    raise ValueError(f\"Newton's method did not converge after {k+1} iterations;\", f\"x={x:.6f}, f(x)={fx:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46400144",
   "metadata": {},
   "source": [
    "Let's try a $3^{rd}$ order degree polynomial, defined as $f(x) = x^3 - 7x^2 + 14x - 8$ and with derivative defined as  $f'(x)=3x^2 - 14x + 14$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01539f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - 7*x**2 + 14*x - 8\n",
    "\n",
    "def fprime(x):\n",
    "    return 3*x**2 - 14*x + 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2823026",
   "metadata": {},
   "source": [
    "Solving using our function `newton`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3767ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal solution is x= 0.9999999967198463\n",
      "number of iterations = 4\n",
      "f(x)= 9.840460890586655e-09\n"
     ]
    }
   ],
   "source": [
    "x, k, sol = newton(f, fprime, 0.7, tol=1e-8, max_iter=50)\n",
    "\n",
    "print('optimal solution is x=', x)\n",
    "print('number of iterations =', k)\n",
    "print('f(x)=', sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a24a2",
   "metadata": {},
   "source": [
    "## 4) Using SciPy: `bisect`, `newton`\n",
    "\n",
    "Scipy already contains implementations of bisection and Newton's method. We will go over the use of these now. But their functionality is no different from what we just saw in class.\n",
    "\n",
    "- `optimize.bisect(f, a, b)`: robust with a bracket.\n",
    "- `optimize.newton(f, x0, fprime=...)`: Newton with derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169330d",
   "metadata": {},
   "source": [
    "Let's try another example, and compare our implementation with scipy's. Let's take a look at $f(x) = x-cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c315be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy bisection, x = 0.7390851332156672\n",
      "SciPy newton, x = 0.7390851332151607\n",
      "Our implementation of bisection, x = 0.7390851331874728\n",
      "Our implementation of Newton's method, x = 0.7390851332151608\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "\n",
    "# Defining the function and its derivative (for Newton's method)\n",
    "def f(x):      return x - np.cos(x)\n",
    "def fprime(x): return 1 + np.sin(x)\n",
    "\n",
    "# Let's solve all of these\n",
    "print('SciPy bisection, x =', optimize.bisect(f, 0.0, 1.0, xtol=1e-12, rtol=1e-12, maxiter=200))\n",
    "print('SciPy newton, x =', optimize.newton(f, x0=0.7, fprime=fprime, tol=1e-12, maxiter=50))\n",
    "print('Our implementation of bisection, x =', bisection(f, 0.0, 1.0)[0])\n",
    "print('Our implementation of Newton\\'s method, x =', newton(f, fprime, x0=0.7)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afedbb0",
   "metadata": {},
   "source": [
    "Now, let's try the example that was used in the demo above for Newton's method, that is $f(x) = x^3 + 7x^2 + 14x -8$.\n",
    "\n",
    "The derivative for this is $f'(x) = 3x^2 - 14x + 14$. \n",
    "\n",
    "This equation has three roots as we saw in the animation above (1, 2 and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b0020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy bisection, x = : 1.0000000000000548\n",
      "SciPy newton, x = : 0.9999999999999999\n",
      "Our implementation of bisection, x = 1.0000000000291038\n",
      "Our implementation of Newton's method, x = 0.9999999999986046\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3 - 7*x**2 + 14*x - 8\n",
    "\n",
    "def fprime(x):\n",
    "    return 3*x**2 - 14*x + 14\n",
    "\n",
    "\n",
    "print('SciPy bisection, x = :', optimize.bisect(f, 0.0, 1.46, xtol=1e-12, rtol=1e-12, maxiter=200))\n",
    "print('SciPy newton, x = :', optimize.newton(f, x0=1.45, fprime=fprime, tol=1e-12, maxiter=50))\n",
    "print('Our implementation of bisection, x =', bisection(f, 0.0, 1.5)[0])\n",
    "print('Our implementation of Newton\\'s method, x =', newton(f, fprime, x0=1.45)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622e7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy bisection, x = : 2.0\n",
      "SciPy newton, x = : 4.0\n",
      "Our implementation of bisection, x = 2.0\n",
      "Our implementation of Newton's method, x = 4.0\n"
     ]
    }
   ],
   "source": [
    "print('SciPy bisection, x = :', optimize.bisect(f, 1.5, 2.5, xtol=1e-12, rtol=1e-12, maxiter=200))\n",
    "print('SciPy newton, x = :', optimize.newton(f, x0=1.5, fprime=fprime, tol=1e-12, maxiter=50))\n",
    "print('Our implementation of bisection, x =', bisection(f, 1.5, 2.5)[0])\n",
    "print('Our implementation of Newton\\'s method, x =', newton(f, fprime, x0=1.5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846039c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy newton, x = : 0.0\n",
      "Our implementation of Newton's method, x = -5.025528739899601e-12\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def fprime(x):\n",
    "    t = np.tanh(x)\n",
    "    return 1.0 - t*t  # sech^2(x)\n",
    "\n",
    "print('SciPy newton, x = :', optimize.newton(f, x0=1.08, fprime=fprime, tol=1e-12, maxiter=50))\n",
    "print('Our implementation of Newton\\'s method, x =', newton(f, fprime, x0=1.01)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42199d45",
   "metadata": {},
   "source": [
    "#### Extension to systems: Newton’s method in higher dimensions\n",
    "\n",
    "It is not uncommon that instead of needing to solve a single nonlinear function, we have a system of nonlinear equations. These arise all the time in chemical reaction engineering, process design and control, process dynamics, and so on. These all share in common the particular feature that now, the unknown is a **vector** $x_k \\in \\mathbb{R}^n$.  \n",
    "We put $n$ nonlinear equations into what we call a vector function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "F(x) =\n",
    "\\begin{bmatrix}\n",
    "f_1(x_1,\\dots,x_n) \\\\\n",
    "\\vdots \\\\\n",
    "f_n(x_1,\\dots,x_n)\n",
    "\\end{bmatrix},\n",
    "\\qquad \n",
    "$$\n",
    "\n",
    "And we want $F(x)=0$. Nothing has changed in our goal (root finding), we now just have more equations to deal with simultaneously. This is actually **a system of nonlinear equations**, and hence, we have to use the appropriate mathematical concepts to deal with this increased dimensionality.\n",
    "\n",
    "#### Jacobian\n",
    "The **Jacobian** $J(x)$ is the matrix of all first derivatives:\n",
    "\n",
    "$$\n",
    "J(x) = \\left[\\frac{\\partial f_i}{\\partial x_j}(x)\\right]_{i,j=1}^n\n",
    "$$\n",
    "\n",
    "Expanding the notation above you can see the full matrix as:\n",
    "\n",
    "$$\n",
    "J(x) = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial f_1}{\\partial x_1}(x) & \\dfrac{\\partial f_1}{\\partial x_2}(x) & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}(x) \\\\\n",
    "\\dfrac{\\partial f_2}{\\partial x_1}(x) & \\dfrac{\\partial f_2}{\\partial x_2}(x) & \\cdots & \\dfrac{\\partial f_2}{\\partial x_n}(x) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial f_n}{\\partial x_1}(x) & \\dfrac{\\partial f_n}{\\partial x_2}(x) & \\cdots & \\dfrac{\\partial f_n}{\\partial x_n}(x)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And here we show that the Newton's method is conceptually the same for the multivariate case. If we expand $F$ employing Taylor around $x_k$:\n",
    "\n",
    "$$\n",
    "F(x) \\approx F(x_k) + J(x_k)(x - x_k)\n",
    "$$\n",
    "\n",
    "And impose the root condition at $x_{k+1}$, that is, we want $F(x_{k+1}) = 0$, we have:\n",
    "\n",
    "$$\n",
    "0 \\approx F(x_k) + J(x_k)(x_{k+1} - x_k)\n",
    "$$\n",
    "\n",
    "Lastly, if we solve for $x_{k+1}$:\n",
    "\n",
    "$$\n",
    "J(x_k)(x_{k+1} - x_k) = -F(x_k),\n",
    "$$\n",
    "$$\n",
    "x_{k+1} = x_k + d_k, \\quad d_k \\text{ solves } J(x_k)d_k = -F(x_k)\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this equation as \n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - J(x_k)^{-1} F(x_k)\n",
    "$$\n",
    "\n",
    "But we avoid this in practice since evaluating an inverse is expensive from a computational standpoint. Instead, we solve the linear system posed by $J(x_k)d_k = -F(x_k)$\n",
    "\n",
    "\n",
    "\n",
    "#### Side-by-side: scalar vs. vector versions of Newton's method\n",
    "\n",
    "To further stress the fact that these are the same algorithm, just adapted for higher dimensions, please take a look at the table below for a quick comparison:\n",
    "\n",
    "| Concept | Scalar (1D) | Vector (nD) |\n",
    "|---|---|---|\n",
    "| Unknown | $x_k \\in \\mathbb{R}$ | $x_k \\in \\mathbb{R}^n$ |\n",
    "| Function | $f:\\mathbb{R}\\to\\mathbb{R}$ | $F:\\mathbb{R}^n\\to\\mathbb{R}^n$ |\n",
    "| Expansion | $f(x)\\approx f(x_k)+f'(x_k)(x-x_k)$ | $F(x)\\approx F(x_k)+J(x_k)(x-x_k)$ |\n",
    "| Root condition | $0\\approx f(x_k)+f'(x_k)(x_{k+1}-x_k)$ | $0\\approx F(x_k)+J(x_k)(x_{k+1}-x_k)$ |\n",
    "| Newton step | $d_k=-\\tfrac{f(x_k)}{f'(x_k)}$ | $J(x_k)d_k=-F(x_k)$ |\n",
    "| Update | $x_{k+1}=x_k+d_k$ | $x_{k+1}=x_k+d_k$ |\n",
    "\n",
    "\n",
    "\n",
    "#### Example: $2\\times 2$ generic system\n",
    "Suppose\n",
    "\n",
    "$$\n",
    "F(x) =\n",
    "\\begin{bmatrix}\n",
    "f_1(x_1,x_2) \\\\\n",
    "f_2(x_1,x_2)\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "J(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Newton step solves\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n",
    "\\end{bmatrix}_{x_k}\n",
    "\\begin{bmatrix}\n",
    "d_{k,1}\\\\\n",
    "d_{k,2}\n",
    "\\end{bmatrix}\n",
    "= -\n",
    "\\begin{bmatrix}\n",
    "f_1(x_k)\\\\\n",
    "f_2(x_k)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then $x_{k+1} = x_k + d_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f544ec",
   "metadata": {},
   "source": [
    "## Can we code this in Python? (We continue from here on Sep 3rd. Your first homework does not require the code development below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b13dad",
   "metadata": {},
   "source": [
    "Yup! It's pretty much the same idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def newton_nd(F, J, x0, tol=1e-8, max_iter = 50):\n",
    "    \"\"\"\n",
    "    Newton's method (multivariate).\n",
    "\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "\n",
    "    f: function\n",
    "        The vector-valued function we want to determine the root.\n",
    "\n",
    "    J: function\n",
    "        The Jacobian of the function F\n",
    "    x0: float\n",
    "        Initial estimate (guess).\n",
    "    tol: float\n",
    "        Tolerance.\n",
    "    max_iter:\n",
    "        Maximum number of iterations...\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    x: float\n",
    "        Approximate root of f(x)\n",
    "    iter: int\n",
    "        Number of iterations\n",
    "    sol: float\n",
    "        Value of f(x) at the solution.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if np.linalg.norm(F(x), ord =2)  < tol:\n",
    "        return x, 0, F(x) \n",
    "    # Main for loop:\n",
    "\n",
    "    for k in range(max_iter):\n",
    "\n",
    "       \n",
    "\n",
    "        x_new = x - np.linalg.inv(J(x)) @ F(x)\n",
    "\n",
    "        if np.linalg.norm(F(x_new)) < tol: # Function value small enough. stop.\n",
    "            return x_new, k+1, F(x_new)\n",
    "        if np.linalg.norm(x_new - x) < 1e-14: # Step size small enough. Stop.\n",
    "            ## TODO: Maybe we should add 1e-14 as a keyword argument instead of hard coding it.\n",
    "            return x_new, k+1, F(x_new)\n",
    "        \n",
    "        x = x_new\n",
    "\n",
    "    \n",
    "    Fx = F(x)\n",
    "    x = x_new\n",
    "\n",
    "    raise ValueError(f\"Newton's method did not converge after {k+1} iterations;\", f\"x={x:.6f}, f(x)={F:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8384c",
   "metadata": {},
   "source": [
    "Let's try this for a system defined as \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2 x_1^2 + x_2^2 = 6 \\\\\n",
    "x_1 + 2 x_2 = 3.5\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f043b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From x0 = [1. 1.] root = [1.5958645  0.95206775] in 5 iters; residual = [8.8817842e-16 0.0000000e+00]\n",
      "From x0 = [-1.   2.2] root = [-0.81808672  2.15904336] in 4 iters; residual = [8.8817842e-16 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Define F and J for the system\n",
    "def F(x):\n",
    "    x1, x2 = x\n",
    "    return np.array([\n",
    "        2.0*x1**2 + x2**2 - 6.0,\n",
    "        x1 + 2.0*x2 - 3.5\n",
    "    ])\n",
    "\n",
    "def J(x):\n",
    "    x1, x2 = x\n",
    "    return np.array([\n",
    "        [4.0*x1, 2.0*x2],\n",
    "        [1.0,    2.0   ]\n",
    "    ])\n",
    "\n",
    "# Two different initial guesses\n",
    "x0_1 = np.array([1.0, 1.0])      \n",
    "x0_2 = np.array([-1.0, 2.2])     \n",
    "\n",
    "root1, it1, res1 = newton_nd(F, J, x0_1, tol=1e-10, max_iter=50)\n",
    "root2, it2, res2 = newton_nd(F, J, x0_2, tol=1e-10, max_iter=50)\n",
    "\n",
    "print(\"From x0 =\", x0_1, \"root =\", root1, \"in\", it1, \"iters; residual =\", res1)\n",
    "print(\"From x0 =\", x0_2, \"root =\", root2, \"in\", it2, \"iters; residual =\", res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78e0f8",
   "metadata": {},
   "source": [
    "# What about using scipy?\n",
    "\n",
    "You can use `scipy.optimize.root` to solve the exact same problem (or any root finding problem). Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7ef378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function root in module scipy.optimize._root:\n",
      "\n",
      "root(\n",
      "    fun,\n",
      "    x0,\n",
      "    args=(),\n",
      "    method='hybr',\n",
      "    jac=None,\n",
      "    tol=None,\n",
      "    callback=None,\n",
      "    options=None\n",
      ")\n",
      "    Find a root of a vector function.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        A vector function to find a root of.\n",
      "\n",
      "        Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "        ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "        Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "        only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "        callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "        gathered before invoking this function.\n",
      "    x0 : ndarray\n",
      "        Initial guess.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its Jacobian.\n",
      "    method : str, optional\n",
      "        Type of solver. Should be one of\n",
      "\n",
      "        - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "        - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "        - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "        - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "        - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "        - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "        - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "        - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "        - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "        - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "\n",
      "    jac : bool or callable, optional\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        value of Jacobian along with the objective function. If False, the\n",
      "        Jacobian will be estimated numerically.\n",
      "        `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "        this case, it must accept the same arguments as `fun`.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    callback : function, optional\n",
      "        Optional callback function. It is called on every iteration as\n",
      "        ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "        the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "        :obj:`show_options()` for details.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    sol : OptimizeResult\n",
      "        The solution represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the algorithm exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    show_options : Additional options accepted by the solvers\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *hybr*.\n",
      "\n",
      "    Method *hybr* uses a modification of the Powell hybrid method as\n",
      "    implemented in MINPACK [1]_.\n",
      "\n",
      "    Method *lm* solves the system of nonlinear equations in a least squares\n",
      "    sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "    implemented in MINPACK [1]_.\n",
      "\n",
      "    Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "\n",
      "    Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "    *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "    with backtracking or full line searches [2]_. Each method corresponds\n",
      "    to a particular Jacobian approximations.\n",
      "\n",
      "    - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "      known as Broyden's good method.\n",
      "    - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "      is known as Broyden's bad method.\n",
      "    - Method *anderson* uses (extended) Anderson mixing.\n",
      "    - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "      is suitable for large-scale problem.\n",
      "    - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "    - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "    - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "      approximation.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        The algorithms implemented for methods *diagbroyden*,\n",
      "        *linearmixing* and *excitingmixing* may be useful for specific\n",
      "        problems, but whether they will work may depend strongly on the\n",
      "        problem.\n",
      "\n",
      "    .. versionadded:: 0.11.0\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "       1980. User Guide for MINPACK-1.\n",
      "    .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "       Equations. Society for Industrial and Applied Mathematics.\n",
      "       <https://archive.siam.org/books/kelley/fr16/>\n",
      "    .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    The following functions define a system of nonlinear equations and its\n",
      "    jacobian.\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def fun(x):\n",
      "    ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "    ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "    >>> def jac(x):\n",
      "    ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "    ...                       -1.5 * (x[0] - x[1])**2],\n",
      "    ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "    ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "\n",
      "    A solution can be obtained as follows.\n",
      "\n",
      "    >>> from scipy import optimize\n",
      "    >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "    >>> sol.x\n",
      "    array([ 0.8411639,  0.1588361])\n",
      "\n",
      "    **Large problem**\n",
      "\n",
      "    Suppose that we needed to solve the following integrodifferential\n",
      "    equation on the square :math:`[0,1]\\times[0,1]`:\n",
      "\n",
      "    .. math::\n",
      "\n",
      "       \\nabla^2 P = 10 \\left(\\int_0^1\\int_0^1\\cosh(P)\\,dx\\,dy\\right)^2\n",
      "\n",
      "    with :math:`P(x,1) = 1` and :math:`P=0` elsewhere on the boundary of\n",
      "    the square.\n",
      "\n",
      "    The solution can be found using the ``method='krylov'`` solver:\n",
      "\n",
      "    >>> from scipy import optimize\n",
      "    >>> # parameters\n",
      "    >>> nx, ny = 75, 75\n",
      "    >>> hx, hy = 1./(nx-1), 1./(ny-1)\n",
      "\n",
      "    >>> P_left, P_right = 0, 0\n",
      "    >>> P_top, P_bottom = 1, 0\n",
      "\n",
      "    >>> def residual(P):\n",
      "    ...    d2x = np.zeros_like(P)\n",
      "    ...    d2y = np.zeros_like(P)\n",
      "    ...\n",
      "    ...    d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n",
      "    ...    d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n",
      "    ...    d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n",
      "    ...\n",
      "    ...    d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n",
      "    ...    d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n",
      "    ...    d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n",
      "    ...\n",
      "    ...    return d2x + d2y - 10*np.cosh(P).mean()**2\n",
      "\n",
      "    >>> guess = np.zeros((nx, ny), float)\n",
      "    >>> sol = optimize.root(residual, guess, method='krylov')\n",
      "    >>> print('Residual: %g' % abs(residual(sol.x)).max())\n",
      "    Residual: 5.7972e-06  # may vary\n",
      "\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> x, y = np.mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n",
      "    >>> plt.pcolormesh(x, y, sol.x, shading='gouraud')\n",
      "    >>> plt.colorbar()\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "help(scipy.optimize.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2e5a7",
   "metadata": {},
   "source": [
    "You can also do `?scipy.optimize.root`. Same idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf409018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "scipy.optimize.root(\n",
      "    fun,\n",
      "    x0,\n",
      "    args=(),\n",
      "    method=\u001b[33m'hybr'\u001b[39m,\n",
      "    jac=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    tol=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    callback=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    options=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Find a root of a vector function.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "fun : callable\n",
      "    A vector function to find a root of.\n",
      "\n",
      "    Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "    ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "    Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "    only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "    callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "    gathered before invoking this function.\n",
      "x0 : ndarray\n",
      "    Initial guess.\n",
      "args : tuple, optional\n",
      "    Extra arguments passed to the objective function and its Jacobian.\n",
      "method : str, optional\n",
      "    Type of solver. Should be one of\n",
      "\n",
      "    - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "    - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "    - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "    - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "    - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "    - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "    - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "    - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "    - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "    - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "\n",
      "jac : bool or callable, optional\n",
      "    If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "    value of Jacobian along with the objective function. If False, the\n",
      "    Jacobian will be estimated numerically.\n",
      "    `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "    this case, it must accept the same arguments as `fun`.\n",
      "tol : float, optional\n",
      "    Tolerance for termination. For detailed control, use solver-specific\n",
      "    options.\n",
      "callback : function, optional\n",
      "    Optional callback function. It is called on every iteration as\n",
      "    ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "    the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "options : dict, optional\n",
      "    A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "    :obj:`show_options()` for details.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "sol : OptimizeResult\n",
      "    The solution represented as a ``OptimizeResult`` object.\n",
      "    Important attributes are: ``x`` the solution array, ``success`` a\n",
      "    Boolean flag indicating if the algorithm exited successfully and\n",
      "    ``message`` which describes the cause of the termination. See\n",
      "    `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "See also\n",
      "--------\n",
      "show_options : Additional options accepted by the solvers\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This section describes the available solvers that can be selected by the\n",
      "'method' parameter. The default method is *hybr*.\n",
      "\n",
      "Method *hybr* uses a modification of the Powell hybrid method as\n",
      "implemented in MINPACK [1]_.\n",
      "\n",
      "Method *lm* solves the system of nonlinear equations in a least squares\n",
      "sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "implemented in MINPACK [1]_.\n",
      "\n",
      "Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "\n",
      "Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "*diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "with backtracking or full line searches [2]_. Each method corresponds\n",
      "to a particular Jacobian approximations.\n",
      "\n",
      "- Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "  known as Broyden's good method.\n",
      "- Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "  is known as Broyden's bad method.\n",
      "- Method *anderson* uses (extended) Anderson mixing.\n",
      "- Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "  is suitable for large-scale problem.\n",
      "- Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "- Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "- Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "  approximation.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "    The algorithms implemented for methods *diagbroyden*,\n",
      "    *linearmixing* and *excitingmixing* may be useful for specific\n",
      "    problems, but whether they will work may depend strongly on the\n",
      "    problem.\n",
      "\n",
      ".. versionadded:: 0.11.0\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "   1980. User Guide for MINPACK-1.\n",
      ".. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "   Equations. Society for Industrial and Applied Mathematics.\n",
      "   <https://archive.siam.org/books/kelley/fr16/>\n",
      ".. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "\n",
      "Examples\n",
      "--------\n",
      "The following functions define a system of nonlinear equations and its\n",
      "jacobian.\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> def fun(x):\n",
      "...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      ">>> def jac(x):\n",
      "...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "...                       -1.5 * (x[0] - x[1])**2],\n",
      "...                      [-1.5 * (x[1] - x[0])**2,\n",
      "...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "\n",
      "A solution can be obtained as follows.\n",
      "\n",
      ">>> from scipy import optimize\n",
      ">>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      ">>> sol.x\n",
      "array([ 0.8411639,  0.1588361])\n",
      "\n",
      "**Large problem**\n",
      "\n",
      "Suppose that we needed to solve the following integrodifferential\n",
      "equation on the square :math:`[0,1]\\times[0,1]`:\n",
      "\n",
      ".. math::\n",
      "\n",
      "   \\nabla^2 P = 10 \\left(\\int_0^1\\int_0^1\\cosh(P)\\,dx\\,dy\\right)^2\n",
      "\n",
      "with :math:`P(x,1) = 1` and :math:`P=0` elsewhere on the boundary of\n",
      "the square.\n",
      "\n",
      "The solution can be found using the ``method='krylov'`` solver:\n",
      "\n",
      ">>> from scipy import optimize\n",
      ">>> # parameters\n",
      ">>> nx, ny = 75, 75\n",
      ">>> hx, hy = 1./(nx-1), 1./(ny-1)\n",
      "\n",
      ">>> P_left, P_right = 0, 0\n",
      ">>> P_top, P_bottom = 1, 0\n",
      "\n",
      ">>> def residual(P):\n",
      "...    d2x = np.zeros_like(P)\n",
      "...    d2y = np.zeros_like(P)\n",
      "...\n",
      "...    d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n",
      "...    d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n",
      "...    d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n",
      "...\n",
      "...    d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n",
      "...    d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n",
      "...    d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n",
      "...\n",
      "...    return d2x + d2y - 10*np.cosh(P).mean()**2\n",
      "\n",
      ">>> guess = np.zeros((nx, ny), float)\n",
      ">>> sol = optimize.root(residual, guess, method='krylov')\n",
      ">>> print('Residual: %g' % abs(residual(sol.x)).max())\n",
      "Residual: 5.7972e-06  # may vary\n",
      "\n",
      ">>> import matplotlib.pyplot as plt\n",
      ">>> x, y = np.mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n",
      ">>> plt.pcolormesh(x, y, sol.x, shading='gouraud')\n",
      ">>> plt.colorbar()\n",
      ">>> plt.show()\n",
      "\u001b[31mFile:\u001b[39m      /opt/anaconda3/envs/numerical/lib/python3.13/site-packages/scipy/optimize/_root.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "?scipy.optimize.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d08c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5958645  0.95206775]\n",
      "[-0.81808672  2.15904336]\n"
     ]
    }
   ],
   "source": [
    "x0_1 = np.array([1.0, 1.0])      \n",
    "x0_2 = np.array([-1, -1])     \n",
    "\n",
    "\n",
    "\n",
    "x_soln_1 = scipy.optimize.root(fun=F, x0=x0_1)\n",
    "print(x_soln_1.x) \n",
    "\n",
    "x_soln_2 = scipy.optimize.root(fun=F, x0=x0_2)\n",
    "print(x_soln_2.x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6a9eb",
   "metadata": {},
   "source": [
    "There is a small detail here. We did not supply a Jacobian to `scipy.optimize.root`. Why do you think that happened? The method needs derivatives/jacobians, but we did not supply one. Why it still worked?\n",
    "\n",
    "But if we do supply a Jacobian, what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0980babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5958645  0.95206775]\n",
      "[1.5958645  0.95206775]\n"
     ]
    }
   ],
   "source": [
    "x_soln_1_jac = scipy.optimize.root(fun=F, x0=x0_1, jac=J)\n",
    "\n",
    "print(x_soln_1.x) \n",
    "print(x_soln_1_jac.x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb19f5",
   "metadata": {},
   "source": [
    "The solutions are the same. But what about number of iterations? Let us print both (without and with) exact Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d8626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: The solution converged.\n",
      " success: True\n",
      "  status: 1\n",
      "     fun: [ 0.000e+00  0.000e+00]\n",
      "       x: [ 1.596e+00  9.521e-01]\n",
      "  method: hybr\n",
      "    nfev: 12\n",
      "    fjac: [[-9.876e-01 -1.572e-01]\n",
      "           [ 1.572e-01 -9.876e-01]]\n",
      "       r: [-6.360e+00 -1.992e+00 -1.708e+00]\n",
      "     qtf: [ 1.086e-10 -1.728e-11]\n"
     ]
    }
   ],
   "source": [
    "print(x_soln_1) # Without providing the Jacobian function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2932ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: The solution converged.\n",
      " success: True\n",
      "  status: 1\n",
      "     fun: [ 0.000e+00  0.000e+00]\n",
      "       x: [ 1.596e+00  9.521e-01]\n",
      "  method: hybr\n",
      "    nfev: 10\n",
      "    njev: 1\n",
      "    fjac: [[-9.876e-01 -1.572e-01]\n",
      "           [ 1.572e-01 -9.876e-01]]\n",
      "       r: [-6.360e+00 -1.992e+00 -1.708e+00]\n",
      "     qtf: [ 1.086e-10 -1.728e-11]\n"
     ]
    }
   ],
   "source": [
    "print(x_soln_1_jac) # Providing the Jacobian function we created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16bba6",
   "metadata": {},
   "source": [
    "⚠️ Important ⚠️\n",
    "\n",
    "It took `scipy.optimize.root` fewer iterations (nfev argument printed in the output, 12 vs 10). Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b45dd",
   "metadata": {},
   "source": [
    "⚠️ Important ⚠️\n",
    "\n",
    "Let's take a look into *derivative approximations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ffa3b",
   "metadata": {},
   "source": [
    "## Wrap-up & references\n",
    "- **Bisection** is reliable in a bracket, but slow. We don't always have a bracket though.\n",
    "- **Newton** is fast with good starting points, but can easily diverge or even find different solutions from what we anticipated (it is based on derivative information).\n",
    "- For systems of nonlinear equations, provide **Jacobians**; `scipy.optimize.root` is robust in practice.\n",
    "\n",
    "\n",
    "**Docs and References used**  \n",
    "- SciPy Optimize:  https://docs.scipy.org/doc/scipy/reference/optimize.html  \n",
    "- PyCSE (Kitchin): https://kitchingroup.cheme.cmu.edu/pycse/  \n",
    "- Harishankar Manikantan, *Solutions of Nonlinear Equations (Chapter 6)*, ECH60 course notes (GitHub: hmanikantan/ECH60): https://github.com/hmanikantan/ECH60\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
